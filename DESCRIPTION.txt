I implemented a Python project with 3 scripts to generate the final results. These scripts use 6 modules I built with the help of 3 libraries:

Scripts:
analyze.py - loads a dataset and creates analysis reports in files
experiment.py - loads, handles and encodes the training set. Also creates a ML model, runs the experiment and save the model to a pickle file
predict.py - loads, handles and encodes the test set. Then, loads a ML model from a previously generated file, makes the predictions and stores it to a CSV

Modules:
IOModule.py - contains the IOProcessor class, which has functions to read and write to files
AnalyzerModule.py - contains the Analyzer class, which creates reports about a dataset
HandlerModule.py - contains the Handler class, which handles missing data and cleans likely unninformative values
EncoderModule.py - contains the Encoder class, which appropriately encodes values for a ML model
ExperimentationModule.py - contains the Experimentation class, which does cross-validation training on the dataset with a ML model
VariablesModule.py - contains several variables used in both experiment.py and predict.py

Libraries:
pandas - to help read and write to CSV, impute or clean values, and encode them
numpy - to recognize data types read by pandas and do statistical calculations
scikit-learn - to create ML models and do cross-validation evaluation

First, I ran analyze.py on the training and testing sets to generate reports to better understand the data. The reports showed the distribution of values per column in the datasets and the original data type of those values. This way, I could decide which columns were informative, which needed imputing for missing values, and how this imputation was going to be made. It also allowed me to check which columns needed encoding.

Then, using a 5-fold cross-validation, I ran experiments with Logistic Regression and Stochastic Gradient Descent Regression (also tuning their hyperparameters) on the training set to check which model had the best results based on the F-Measure score per fold, the average of them and the standard deviation.

Finally, the probabilities were generated using the model with a good balance between having a high average F-Measure while maintaining a small deviation.